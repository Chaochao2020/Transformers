{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于截断策略的机器阅读理解任务实现"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-26T02:57:27.421780Z",
     "start_time": "2023-12-26T02:57:27.290058700Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DefaultDataCollator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-26T02:57:30.396616500Z",
     "start_time": "2023-12-26T02:57:30.207777400Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file: 'mrc_data/dataset_dict.json'. Expected to load a `DatasetDict` object, but provided path is not a `DatasetDict`.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_1489035/2472591537.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;31m#datasets = load_dataset(\"cmrc2018\", cache_dir=\"data\")\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# 如果无法联网，则使用下面的方式加载数据集\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mdatasets\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDatasetDict\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_from_disk\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"mrc_data\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/czh_torch/lib/python3.7/site-packages/datasets/dataset_dict.py\u001B[0m in \u001B[0;36mload_from_disk\u001B[0;34m(dataset_dict_path, fs, keep_in_memory, storage_options)\u001B[0m\n\u001B[1;32m   1356\u001B[0m                 )\n\u001B[1;32m   1357\u001B[0m             raise FileNotFoundError(\n\u001B[0;32m-> 1358\u001B[0;31m                 \u001B[0;34mf\"No such file: '{dataset_dict_json_path}'. Expected to load a `DatasetDict` object, but provided path is not a `DatasetDict`.\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1359\u001B[0m             )\n\u001B[1;32m   1360\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: No such file: 'mrc_data/dataset_dict.json'. Expected to load a `DatasetDict` object, but provided path is not a `DatasetDict`."
     ]
    }
   ],
   "source": [
    "# 如果可以联网，直接使用load_dataset进行加载\n",
    "#datasets = load_dataset(\"cmrc2018\", cache_dir=\"data\")\n",
    "# 如果无法联网，则使用下面的方式加载数据集\n",
    "datasets = DatasetDict.load_from_disk(\"mrc_data\")\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-26T02:56:33.713828700Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-26T02:56:33.716828400Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-26T02:56:33.719828400Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_dataset = datasets[\"train\"].select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-26T02:56:33.723829800Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_examples = tokenizer(text=sample_dataset[\"question\"],\n",
    "                               text_pair=sample_dataset[\"context\"],\n",
    "                               return_offsets_mapping=True,\n",
    "                               max_length=512, truncation=\"only_second\", padding=\"max_length\")\n",
    "tokenized_examples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-26T02:56:33.724843200Z"
    }
   },
   "outputs": [],
   "source": [
    "print(tokenized_examples[\"offset_mapping\"][0], len(tokenized_examples[\"offset_mapping\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-26T02:56:33.731842100Z",
     "start_time": "2023-12-26T02:56:33.724843200Z"
    }
   },
   "outputs": [],
   "source": [
    "offset_mapping = tokenized_examples.pop(\"offset_mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-26T02:56:33.726839200Z"
    }
   },
   "outputs": [],
   "source": [
    "for idx, offset in enumerate(offset_mapping):\n",
    "    answer = sample_dataset[idx][\"answers\"]\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = start_char + len(answer[\"text\"][0])\n",
    "    # 定位答案在token中的起始位置和结束位置\n",
    "    # 一种策略，我们要拿到context的起始和结束，然后从左右两侧向答案逼近\n",
    "\n",
    "    context_start = tokenized_examples.sequence_ids(idx).index(1)\n",
    "    context_end = tokenized_examples.sequence_ids(idx).index(None, context_start) - 1\n",
    "\n",
    "    # 判断答案是否在context中\n",
    "    if offset[context_end][1] < start_char or offset[context_start][0] > end_char:\n",
    "        start_token_pos = 0\n",
    "        end_token_pos = 0\n",
    "    else:\n",
    "        token_id = context_start\n",
    "        while token_id <= context_end and offset[token_id][0] < start_char:\n",
    "            token_id += 1\n",
    "        start_token_pos = token_id\n",
    "        token_id = context_end\n",
    "        while token_id >= context_start and offset[token_id][1] > end_char:\n",
    "            token_id -=1\n",
    "        end_token_pos = token_id\n",
    "        \n",
    "    print(answer, start_char, end_char, context_start, context_end, start_token_pos, end_token_pos)\n",
    "    print(\"token answer decode:\", tokenizer.decode(tokenized_examples[\"input_ids\"][idx][start_token_pos: end_token_pos + 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-26T02:56:33.729829500Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_func(examples):\n",
    "    tokenized_examples = tokenizer(text=examples[\"question\"],\n",
    "                               text_pair=examples[\"context\"],\n",
    "                               return_offsets_mapping=True,\n",
    "                               max_length=384, truncation=\"only_second\", padding=\"max_length\")\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for idx, offset in enumerate(offset_mapping):\n",
    "        answer = examples[\"answers\"][idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "        # 定位答案在token中的起始位置和结束位置\n",
    "        # 一种策略，我们要拿到context的起始和结束，然后从左右两侧向答案逼近\n",
    "        context_start = tokenized_examples.sequence_ids(idx).index(1)\n",
    "        context_end = tokenized_examples.sequence_ids(idx).index(None, context_start) - 1\n",
    "        # 判断答案是否在context中\n",
    "        if offset[context_end][1] < start_char or offset[context_start][0] > end_char:\n",
    "            start_token_pos = 0\n",
    "            end_token_pos = 0\n",
    "        else:\n",
    "            token_id = context_start\n",
    "            while token_id <= context_end and offset[token_id][0] < start_char:\n",
    "                token_id += 1\n",
    "            start_token_pos = token_id\n",
    "            token_id = context_end\n",
    "            while token_id >= context_start and offset[token_id][1] > end_char:\n",
    "                token_id -=1\n",
    "            end_token_pos = token_id\n",
    "        start_positions.append(start_token_pos)\n",
    "        end_positions.append(end_token_pos)\n",
    "    \n",
    "    tokenized_examples[\"start_positions\"] = start_positions\n",
    "    tokenized_examples[\"end_positions\"] = end_positions\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-26T02:56:33.780355400Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenied_datasets = datasets.map(process_func, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
    "tokenied_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-26T02:56:33.780355400Z"
    }
   },
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"hfl/chinese-macbert-base\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 配置TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-26T02:56:33.780355400Z"
    }
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"models_for_qa\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=3\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 配置Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-26T02:56:33.780355400Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenied_datasets[\"train\"],\n",
    "    eval_dataset=tokenied_datasets[\"validation\"],\n",
    "    data_collator=DefaultDataCollator()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-26T02:56:33.780355400Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-26T02:56:33.781355800Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-26T02:56:33.781355800Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe(question=\"小明在哪里上班？\", context=\"小明在北京上班。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-26T02:56:33.781355800Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
